{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:27:10.820208Z",
     "start_time": "2025-06-18T14:27:10.078854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import tempfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from huggingface_hub import list_models, model_info, hf_hub_download\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "b55518abe8917fa1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/miniconda3/envs/NLP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Acquisition: Building the Dataset through the Hugging Face Hub API\n",
    "\n",
    "### Overview\n",
    "The goal of this step is to construct a robust and informative dataset containing metadata for every model hosted on the Hugging Face Hub. This dataset will serve as a ground truth for a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "### The Hugging Face Hub API\n",
    "The [Hugging Face Hub API](https://huggingface.co/docs/huggingface_hub/package_reference/hf_api) provides programmatic access to the platform's extensive model repository through several endpoints, each serving different purposes and offering varying levels of detail.<br>\n",
    "The `list_models()` method allows iteration over ModelInfo objects but does not return complete metadata, even with the <b>full=True</b> parameter. To access all relevant information, we must call [`model_info()`](https://huggingface.co/docs/huggingface_hub/v0.32.3/en/package_reference/hf_api#huggingface_hub.ModelInfo) individually for each model ID. This yields full metadata, but at the cost of a large number of API requests, which can impact performance and scalability.\n",
    "\n",
    "----\n",
    "\n",
    "### Metadata Fields\n",
    "The fields that we can retrieve and those that are most useful for our purposes include:\n",
    "- **model_id**: Unique identifier for the model.\n",
    "- **base_model**: Identifier of the base model from which this model derives (e.g., for fine-tuned models).\n",
    "- **author**: The creator or organization behind the model.\n",
    "- **license**: Licensing information for the model.\n",
    "- **language**: Language of the model's training data or metadata.\n",
    "- **downloads**: Number of times the model has been downloaded.\n",
    "- **likes**: Number of likes the model has received.\n",
    "- **tags**: Tags associated with the model for easier categorization.\n",
    "- **pipeline_tag**: The pipeline tag associated with the model (e.g., text-generation, image-classification).\n",
    "- **library_name**: The library name associated with the model (e.g., transformers, diffusers).\n",
    "- **created_at**: Timestamp of when the model was created.\n",
    "- **readme_file**: The readme file of the model repository, which may contain additional context and information about the model.\n",
    "\n",
    "---\n",
    "\n",
    "### The Challenge of Context\n",
    "While the metadata fields provide valuable insights, they often lack sufficient context to fully understand the model's capabilities, limitations, and training methodology. The readme file of each model repository is a crucial resource for this additional context, but it comes with its own set of challenges:\n",
    "- **Inconsistency**: Not all models have a readme file, and those that do may vary significantly in content quality and relevance.\n",
    "- **Information Overload**: Some readme files may contain excessive or irrelevant information, making it difficult to extract useful insights.\n",
    "- **Lack of Control**: The content of readme files is user-generated, so we cannot guarantee the presence or quality of information.\n",
    "- **Performance**: Downloading readme files for a large number of models can be time-consuming and resource-intensive."
   ],
   "id": "f08d5af020b34c1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:29:53.281852Z",
     "start_time": "2025-06-18T14:29:53.277709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "HF_TOKEN = \"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\"\n",
    "MAX_WORKERS = 5\n",
    "BATCH_SIZE = 100\n",
    "CHECKPOINT_FILE = \"models_scraping_checkpoint.json\"\n",
    "MODEL_LIMIT = 1100\n",
    "# Cache Directory\n",
    "CHACE_DIR = \"temp_cache\"\n",
    "os.makedirs(CHACE_DIR, exist_ok=True)\n",
    "TEMP_CACHE_DIR = tempfile.mkdtemp(prefix=\"hf_temp_cache_\", dir=CHACE_DIR)\n",
    "# Generating Embeddings Batch Size\n",
    "EMBEDDINGS_BATCH_SIZE = 200"
   ],
   "id": "3b97a71aa6e3a2a9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:29:56.521336Z",
     "start_time": "2025-06-18T14:29:56.505609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_readme_from_repository(repository_id: str):\n",
    "    \"\"\"\n",
    "    Fetches the README.md file from a given Hugging Face dataset repository.\n",
    "\n",
    "    Args:\n",
    "        repository_id (str): The ID of the Hugging Face dataset repository.\n",
    "    Returns:\n",
    "        str: The content of the README.md file, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        readme_content = hf_hub_download(\n",
    "            repo_id=repository_id,\n",
    "            filename=\"README.md\",\n",
    "            token=HF_TOKEN,\n",
    "            cache_dir=TEMP_CACHE_DIR\n",
    "        )\n",
    "\n",
    "        with open(readme_content, \"r\", encoding=\"utf-8\") as f:\n",
    "            readme_text = f.read()\n",
    "\n",
    "        return readme_text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download README for {repository_id}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def process_single_model(model_id: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Processes a single model metadata from the Hugging Face Hub.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): The ID of the model to process.\n",
    "    Returns:\n",
    "        Optional[Dict]: A dictionary containing the processed metadata, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info = model_info(model_id, token=HF_TOKEN)\n",
    "        card_data = info.cardData if hasattr(info, 'cardData') and info.cardData else {}\n",
    "\n",
    "        readme = get_readme_from_repository(model_id)\n",
    "\n",
    "        return {\n",
    "            'model_id': model_id,\n",
    "            'base_model': getattr(card_data, 'base_model', None),\n",
    "            'author': getattr(info, 'author', None),\n",
    "            'readme_file': readme,\n",
    "            'license': getattr(card_data, 'license', None),\n",
    "            'language': getattr(card_data, 'language', None),\n",
    "            'downloads': getattr(info, 'downloads', 0),\n",
    "            'likes': getattr(info, 'likes', 0),\n",
    "            'tags': ', '.join(info.tags) if hasattr(info, 'tags') and info.tags else '',\n",
    "            'pipeline_tag': getattr(info, 'pipeline_tag', None),\n",
    "            'library_name': getattr(info, 'library_name', None),\n",
    "            'created_at': getattr(info, 'created_at', None),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {model_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_batch_threaded(model_ids: List[str]) -> List[Dict]:\n",
    "    \"\"\"Process a batch of models using ThreadPoolExecutor\"\"\"\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_model = {\n",
    "            executor.submit(process_single_model, model_id): model_id\n",
    "            for model_id in model_ids\n",
    "        }\n",
    "\n",
    "        # Collect results with progress bar\n",
    "        for future in tqdm(as_completed(future_to_model),\n",
    "                           total=len(model_ids),\n",
    "                           desc=f\"Processing batch\"):\n",
    "            try:\n",
    "                result = future.result(timeout=60)  # 60 second timeout\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                model_id = future_to_model[future]\n",
    "                logger.error(f\"Timeout/Error for {model_id}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_checkpoint(data):\n",
    "    \"\"\"\n",
    "    Saves the processed data to a JSON checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of processed dataset metadata dictionaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'w') as f:\n",
    "            json.dump(data, f, indent=2, default=str)\n",
    "        logging.info(f\"Checkpoint saved: {len(data)} datasets processed\")\n",
    "        print(f\"Progress: {len(data)}/{len(datasets)} datasets completed\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save checkpoint: {e}\")\n",
    "\n",
    "\n",
    "def cleanup_temp_cache():\n",
    "    \"\"\"\n",
    "    Cleans up the temporary cache directory used for storing readme files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(TEMP_CACHE_DIR):\n",
    "            shutil.rmtree(TEMP_CACHE_DIR)\n",
    "            logging.info(f\"Current temporary cache {TEMP_CACHE_DIR} deleted.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not delete current temp cache dir {TEMP_CACHE_DIR}: {e}\")\n",
    "\n",
    "\n",
    "def clean_all_cache_folders():\n",
    "    \"\"\"\n",
    "    Cleans up all cache folders in the CHACE_DIR directory.\n",
    "    This function deletes all files and directories within the CHACE_DIR.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(CHACE_DIR):\n",
    "            for item in os.listdir(CHACE_DIR):\n",
    "                item_path = os.path.join(CHACE_DIR, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    shutil.rmtree(item_path)\n",
    "                    logging.info(f\"Deleted cache folder: {item_path}\")\n",
    "                else:\n",
    "                    os.remove(item_path)\n",
    "                    logging.info(f\"Deleted cache file: {item_path}\")\n",
    "\n",
    "            logging.info(f\"All cache folders in {CHACE_DIR} cleaned up.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not delete cache folders in {CHACE_DIR}: {e}\")"
   ],
   "id": "7daa8949e8e5fdd5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:30:02.172506Z",
     "start_time": "2025-06-18T14:29:59.466264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Checkpoint (if exists)\n",
    "checkpoint_data = []\n",
    "start_index = 0\n",
    "\n",
    "if Path(CHECKPOINT_FILE).exists():\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        start_index = len(checkpoint_data)\n",
    "        print(f\"Loaded checkpoint: {start_index} models already processed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load checkpoint file: {e}\")\n",
    "else:\n",
    "    print(\"No checkpoint file found. Starting from scratch.\")"
   ],
   "id": "fb070de8af96d7a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: 169986 models already processed\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:30:24.157458Z",
     "start_time": "2025-06-18T14:30:23.139511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Fetching models from Hugging Face Hub...\")\n",
    "models = list(list_models(limit=MODEL_LIMIT))\n",
    "print(f\"Fetched {len(models)} models\")\n",
    "\n",
    "# Prepare models to process\n",
    "models_to_process = models[start_index:]\n",
    "all_data = checkpoint_data.copy()\n",
    "\n",
    "print(f\"Models remaining to process: {len(models_to_process)}\")"
   ],
   "id": "c2ff60bfbc58d3cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching models from Hugging Face Hub...\n",
      "Fetched 1100 models\n",
      "Models remaining to process: 0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Process Models in Batches\n",
    "if models_to_process:\n",
    "    total_batches = (len(models_to_process) - 1) // BATCH_SIZE + 1\n",
    "\n",
    "    for i in range(0, len(models_to_process), BATCH_SIZE):\n",
    "        batch_models = models_to_process[i:i + BATCH_SIZE]\n",
    "        batch_ids = [m.modelId for m in batch_models]\n",
    "\n",
    "        current_batch = i // BATCH_SIZE + 1\n",
    "        print(f\"\\nProcessing batch {current_batch}/{total_batches}\")\n",
    "        print(f\"Batch size: {len(batch_ids)} models\")\n",
    "\n",
    "        # Process batch\n",
    "        batch_results = process_batch_threaded(batch_ids)\n",
    "        all_data.extend(batch_results)\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(all_data)\n",
    "\n",
    "        # Clean up temporary temp_cache directory\n",
    "        cleanup_temp_cache()\n",
    "\n",
    "        # Rate limiting\n",
    "        time.sleep(0.5)"
   ],
   "id": "62b30884a9d51fd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:30:28.059769Z",
     "start_time": "2025-06-18T14:30:27.540516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"\\nScraping completed!\")\n",
    "print(f\"Total models processed: {len(df)}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ],
   "id": "cfff885db55dba8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completed!\n",
      "Total models processed: 169986\n",
      "Dataset shape: (169986, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                model_id  \\\n",
       "0         Qwen/Qwen3-Embedding-0.6B-GGUF   \n",
       "1            fishaudio/openaudio-s1-mini   \n",
       "2           deepseek-ai/DeepSeek-R1-0528   \n",
       "3         mistralai/Magistral-Small-2506   \n",
       "4  google/gemma-3n-E4B-it-litert-preview   \n",
       "\n",
       "                                        base_model       author  \\\n",
       "0                           [Qwen/Qwen3-0.6B-Base]         Qwen   \n",
       "1                                             None    fishaudio   \n",
       "2                                             None  deepseek-ai   \n",
       "3  [mistralai/Mistral-Small-3.1-24B-Instruct-2503]    mistralai   \n",
       "4                                             None       google   \n",
       "\n",
       "                                         readme_file          license  \\\n",
       "0  ---\\nlicense: apache-2.0\\nbase_model:\\n- Qwen/...       apache-2.0   \n",
       "1  ---\\ntags:\\n- text-to-speech\\nlicense: cc-by-n...  cc-by-nc-sa-4.0   \n",
       "2  ---\\nlicense: mit\\nlibrary_name: transformers\\...              mit   \n",
       "3  ---\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- pt\\n...       apache-2.0   \n",
       "4  ---\\nlicense: gemma\\npipeline_tag: image-text-...            gemma   \n",
       "\n",
       "                                            language  downloads  likes  \\\n",
       "0                                               None      10264    307   \n",
       "1  [zh, en, de, ja, fr, es, ko, ar, nl, ru, it, p...       1649    204   \n",
       "2                                               None     104615   1918   \n",
       "3  [en, fr, de, es, pt, it, ja, ko, ru, zh, ar, f...        101    231   \n",
       "4                                               None          0   1074   \n",
       "\n",
       "                                                tags        pipeline_tag  \\\n",
       "0  gguf, arxiv:2506.05176, base_model:Qwen/Qwen3-...                None   \n",
       "1  dual_ar, text-to-speech, zh, en, de, ja, fr, e...      text-to-speech   \n",
       "2  transformers, safetensors, deepseek_v3, text-g...     text-generation   \n",
       "3  vllm, safetensors, mistral, conversational, en...     text-generation   \n",
       "4  image-text-to-text, arxiv:1905.07830, arxiv:19...  image-text-to-text   \n",
       "\n",
       "   library_name                 created_at  \n",
       "0          None  2025-06-05 08:34:51+00:00  \n",
       "1          None  2025-05-31 11:57:47+00:00  \n",
       "2  transformers  2025-05-28 09:46:42+00:00  \n",
       "3          vllm  2025-06-04 10:51:21+00:00  \n",
       "4          None  2025-05-18 19:24:14+00:00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>base_model</th>\n",
       "      <th>author</th>\n",
       "      <th>readme_file</th>\n",
       "      <th>license</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>tags</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>library_name</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen/Qwen3-Embedding-0.6B-GGUF</td>\n",
       "      <td>[Qwen/Qwen3-0.6B-Base]</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>---\\nlicense: apache-2.0\\nbase_model:\\n- Qwen/...</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>10264</td>\n",
       "      <td>307</td>\n",
       "      <td>gguf, arxiv:2506.05176, base_model:Qwen/Qwen3-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-06-05 08:34:51+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fishaudio/openaudio-s1-mini</td>\n",
       "      <td>None</td>\n",
       "      <td>fishaudio</td>\n",
       "      <td>---\\ntags:\\n- text-to-speech\\nlicense: cc-by-n...</td>\n",
       "      <td>cc-by-nc-sa-4.0</td>\n",
       "      <td>[zh, en, de, ja, fr, es, ko, ar, nl, ru, it, p...</td>\n",
       "      <td>1649</td>\n",
       "      <td>204</td>\n",
       "      <td>dual_ar, text-to-speech, zh, en, de, ja, fr, e...</td>\n",
       "      <td>text-to-speech</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-05-31 11:57:47+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek-ai/DeepSeek-R1-0528</td>\n",
       "      <td>None</td>\n",
       "      <td>deepseek-ai</td>\n",
       "      <td>---\\nlicense: mit\\nlibrary_name: transformers\\...</td>\n",
       "      <td>mit</td>\n",
       "      <td>None</td>\n",
       "      <td>104615</td>\n",
       "      <td>1918</td>\n",
       "      <td>transformers, safetensors, deepseek_v3, text-g...</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>transformers</td>\n",
       "      <td>2025-05-28 09:46:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistralai/Magistral-Small-2506</td>\n",
       "      <td>[mistralai/Mistral-Small-3.1-24B-Instruct-2503]</td>\n",
       "      <td>mistralai</td>\n",
       "      <td>---\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- pt\\n...</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[en, fr, de, es, pt, it, ja, ko, ru, zh, ar, f...</td>\n",
       "      <td>101</td>\n",
       "      <td>231</td>\n",
       "      <td>vllm, safetensors, mistral, conversational, en...</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>vllm</td>\n",
       "      <td>2025-06-04 10:51:21+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google/gemma-3n-E4B-it-litert-preview</td>\n",
       "      <td>None</td>\n",
       "      <td>google</td>\n",
       "      <td>---\\nlicense: gemma\\npipeline_tag: image-text-...</td>\n",
       "      <td>gemma</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1074</td>\n",
       "      <td>image-text-to-text, arxiv:1905.07830, arxiv:19...</td>\n",
       "      <td>image-text-to-text</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-05-18 19:24:14+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cleaning Markdown Readme Files\n",
    "This step involves cleaning the readme files extracted from the Hugging Face models to ensure that they contain only relevant textual content, while preserving titles and important information. The cleaning process will remove unnecessary formatting, images, links, and other non-essential elements."
   ],
   "id": "2329f6cca3134533"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:30:50.829509Z",
     "start_time": "2025-06-18T14:30:35.296419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_markdown(text):\n",
    "    # Remove YAML front matter\n",
    "    text = re.sub(r'^---.*?---\\s*', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove images (![alt](url))\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Remove markdown links but keep the visible text: [text](url) → text\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "\n",
    "    # Remove tables (lines containing |, excluding bullet points)\n",
    "    text = re.sub(r'^\\s*\\|.*\\|.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove code blocks (``` ... ```)\n",
    "    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove inline code (`code`)\n",
    "    text = re.sub(r'`[^`]+`', '', text)\n",
    "\n",
    "    # Remove blockquotes (> ...)\n",
    "    text = re.sub(r'^\\s*>.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove citation block (```)\n",
    "    text = re.sub(r'^@misc.*?```', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove extra newlines\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "\n",
    "    # Trim whitespace\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Clean the readme text in the DataFrame\n",
    "df['readme_file'] = df['readme_file'].apply(clean_markdown)"
   ],
   "id": "4f7507a9b6fcef4c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:30:58.400344Z",
     "start_time": "2025-06-18T14:30:58.304854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Readme empty string count:\", (df['readme_file'] == '').sum())\n",
    "print(\"Removing models with empty readme...\")\n",
    "df = df[df['readme_file'] != '']\n",
    "print(\"DataFrame shape after removing empty readmes:\", df.shape)\n",
    "print(\"Applying markdown cleaning to readme files\")\n",
    "#df['readme_file'] = df['readme_file'].apply(clean_markdown)\n",
    "print(\"Final shape of DataFrame:\", df.shape)\n",
    "df.head()"
   ],
   "id": "adcbb2302c5ee22c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readme empty string count: 81827\n",
      "Removing models with empty readme...\n",
      "DataFrame shape after removing empty readmes: (88159, 12)\n",
      "Applying markdown cleaning to readme files\n",
      "Final shape of DataFrame: (88159, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                model_id  \\\n",
       "0         Qwen/Qwen3-Embedding-0.6B-GGUF   \n",
       "1            fishaudio/openaudio-s1-mini   \n",
       "2           deepseek-ai/DeepSeek-R1-0528   \n",
       "3         mistralai/Magistral-Small-2506   \n",
       "4  google/gemma-3n-E4B-it-litert-preview   \n",
       "\n",
       "                                        base_model       author  \\\n",
       "0                           [Qwen/Qwen3-0.6B-Base]         Qwen   \n",
       "1                                             None    fishaudio   \n",
       "2                                             None  deepseek-ai   \n",
       "3  [mistralai/Mistral-Small-3.1-24B-Instruct-2503]    mistralai   \n",
       "4                                             None       google   \n",
       "\n",
       "                                         readme_file          license  \\\n",
       "0  # Qwen3-Embedding-0.6B-GGUF\\n\\n    \\n\\n## High...       apache-2.0   \n",
       "1  # OpenAudio S1\\n\\n**OpenAudio S1** is a leadin...  cc-by-nc-sa-4.0   \n",
       "2  # DeepSeek-R1-0528\\n\\n  \\n\\n  \\n    \\n  \\n  \\n...              mit   \n",
       "3  # Model Card for Magistral-Small-2506\\n\\nBuild...       apache-2.0   \n",
       "4  # Gemma 3n model card\\n\\n**Model Page**: Gemma...            gemma   \n",
       "\n",
       "                                            language  downloads  likes  \\\n",
       "0                                               None      10264    307   \n",
       "1  [zh, en, de, ja, fr, es, ko, ar, nl, ru, it, p...       1649    204   \n",
       "2                                               None     104615   1918   \n",
       "3  [en, fr, de, es, pt, it, ja, ko, ru, zh, ar, f...        101    231   \n",
       "4                                               None          0   1074   \n",
       "\n",
       "                                                tags        pipeline_tag  \\\n",
       "0  gguf, arxiv:2506.05176, base_model:Qwen/Qwen3-...                None   \n",
       "1  dual_ar, text-to-speech, zh, en, de, ja, fr, e...      text-to-speech   \n",
       "2  transformers, safetensors, deepseek_v3, text-g...     text-generation   \n",
       "3  vllm, safetensors, mistral, conversational, en...     text-generation   \n",
       "4  image-text-to-text, arxiv:1905.07830, arxiv:19...  image-text-to-text   \n",
       "\n",
       "   library_name                 created_at  \n",
       "0          None  2025-06-05 08:34:51+00:00  \n",
       "1          None  2025-05-31 11:57:47+00:00  \n",
       "2  transformers  2025-05-28 09:46:42+00:00  \n",
       "3          vllm  2025-06-04 10:51:21+00:00  \n",
       "4          None  2025-05-18 19:24:14+00:00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>base_model</th>\n",
       "      <th>author</th>\n",
       "      <th>readme_file</th>\n",
       "      <th>license</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>tags</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>library_name</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen/Qwen3-Embedding-0.6B-GGUF</td>\n",
       "      <td>[Qwen/Qwen3-0.6B-Base]</td>\n",
       "      <td>Qwen</td>\n",
       "      <td># Qwen3-Embedding-0.6B-GGUF\\n\\n    \\n\\n## High...</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>10264</td>\n",
       "      <td>307</td>\n",
       "      <td>gguf, arxiv:2506.05176, base_model:Qwen/Qwen3-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-06-05 08:34:51+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fishaudio/openaudio-s1-mini</td>\n",
       "      <td>None</td>\n",
       "      <td>fishaudio</td>\n",
       "      <td># OpenAudio S1\\n\\n**OpenAudio S1** is a leadin...</td>\n",
       "      <td>cc-by-nc-sa-4.0</td>\n",
       "      <td>[zh, en, de, ja, fr, es, ko, ar, nl, ru, it, p...</td>\n",
       "      <td>1649</td>\n",
       "      <td>204</td>\n",
       "      <td>dual_ar, text-to-speech, zh, en, de, ja, fr, e...</td>\n",
       "      <td>text-to-speech</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-05-31 11:57:47+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek-ai/DeepSeek-R1-0528</td>\n",
       "      <td>None</td>\n",
       "      <td>deepseek-ai</td>\n",
       "      <td># DeepSeek-R1-0528\\n\\n  \\n\\n  \\n    \\n  \\n  \\n...</td>\n",
       "      <td>mit</td>\n",
       "      <td>None</td>\n",
       "      <td>104615</td>\n",
       "      <td>1918</td>\n",
       "      <td>transformers, safetensors, deepseek_v3, text-g...</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>transformers</td>\n",
       "      <td>2025-05-28 09:46:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistralai/Magistral-Small-2506</td>\n",
       "      <td>[mistralai/Mistral-Small-3.1-24B-Instruct-2503]</td>\n",
       "      <td>mistralai</td>\n",
       "      <td># Model Card for Magistral-Small-2506\\n\\nBuild...</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>[en, fr, de, es, pt, it, ja, ko, ru, zh, ar, f...</td>\n",
       "      <td>101</td>\n",
       "      <td>231</td>\n",
       "      <td>vllm, safetensors, mistral, conversational, en...</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>vllm</td>\n",
       "      <td>2025-06-04 10:51:21+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google/gemma-3n-E4B-it-litert-preview</td>\n",
       "      <td>None</td>\n",
       "      <td>google</td>\n",
       "      <td># Gemma 3n model card\\n\\n**Model Page**: Gemma...</td>\n",
       "      <td>gemma</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1074</td>\n",
       "      <td>image-text-to-text, arxiv:1905.07830, arxiv:19...</td>\n",
       "      <td>image-text-to-text</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-05-18 19:24:14+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generating Content Embeddings\n",
    "This is a preprocessing step in order to generate embeddings for the content of the models. The embeddings will be used to compare and rank models based on their metadata and readme content, enabling efficient retrieval in a RAG system. The model used is an open source model: [jinaai/jina-embeddings-v3](https://huggingface.co/jinaai/jina-embeddings-v3)\n",
    "\n",
    "A few important considerations:\n",
    "- The embedding model is not specifically trained on structured metadata fields (e.g. license, tags), so it may not fully capture their semantic weight or relevance.\n",
    "- README files often contain noisy, inconsistent, or sparse information. This can affect the quality of the resulting embeddings.\n",
    "- Some fields such as `license`,`language` or `tags` might be missing or incomplete.\n",
    "\n",
    "\n",
    "### Weighted Embeddings Approach\n",
    "In this section, we will generate embeddings for the datasets using a weighted approach. Each field in the dataset will be assigned a weight based on its importance for content matching and retrieval. The weights will be used to combine the embeddings of different fields into a single embedding vector."
   ],
   "id": "ae644642af27146d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:31:28.675178Z",
     "start_time": "2025-06-18T14:31:26.553174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"Output/models_hg_cleaned.csv\")\n",
    "df = df.sample(30000, random_state=42).reset_index(drop=True)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df.columns)"
   ],
   "id": "a7700e010c8e543f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (30000, 12)\n",
      "Index(['model_id', 'base_model', 'author', 'readme_file', 'license',\n",
      "       'language', 'downloads', 'likes', 'tags', 'pipeline_tag',\n",
      "       'library_name', 'created_at'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T14:31:31.126692Z",
     "start_time": "2025-06-18T14:31:31.114680Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "dd8e5b7222bcdaf0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            model_id base_model      author  \\\n",
       "0                                 pnparam/xlsr_comb2        NaN     pnparam   \n",
       "1                                       Wlad777/test        NaN     Wlad777   \n",
       "2                                   VAZaytsev/ppo_LL        NaN   VAZaytsev   \n",
       "3  cardiffnlp/twitter-xlm-roberta-base-sentiment-...        NaN  cardiffnlp   \n",
       "4                 alireza7/ARMAN-SS-100-persian-base        NaN    alireza7   \n",
       "\n",
       "                                         readme_file     license language  \\\n",
       "0  # xlsr_comb2\\n\\nThis model is a fine-tuned ver...  apache-2.0      NaN   \n",
       "1                                               10+9         NaN      NaN   \n",
       "2  # PPO Agent Playing LunarLander-v2\\n\\n  This i...         NaN      NaN   \n",
       "3  # cardiffnlp/twitter-xlm-roberta-base-sentimen...         NaN      NaN   \n",
       "4   More information about models is available here.         NaN      NaN   \n",
       "\n",
       "   downloads  likes                                               tags  \\\n",
       "0          7      0  transformers, pytorch, wav2vec2, automatic-spe...   \n",
       "1          0      0                                          region:us   \n",
       "2          0      0  tensorboard, LunarLander-v2, ppo, deep-reinfor...   \n",
       "3      48289     28  transformers, pytorch, xlm-roberta, text-class...   \n",
       "4         21      0  transformers, pytorch, pegasus, text2text-gene...   \n",
       "\n",
       "                   pipeline_tag  library_name                 created_at  \n",
       "0  automatic-speech-recognition  transformers  2023-02-25 15:37:17+00:00  \n",
       "1                           NaN           NaN  2023-02-23 08:04:39+00:00  \n",
       "2        reinforcement-learning           NaN  2023-03-12 14:24:05+00:00  \n",
       "3           text-classification  transformers  2022-12-01 00:32:11+00:00  \n",
       "4          text2text-generation  transformers  2022-03-02 23:29:05+00:00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>base_model</th>\n",
       "      <th>author</th>\n",
       "      <th>readme_file</th>\n",
       "      <th>license</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>tags</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>library_name</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pnparam/xlsr_comb2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pnparam</td>\n",
       "      <td># xlsr_comb2\\n\\nThis model is a fine-tuned ver...</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers, pytorch, wav2vec2, automatic-spe...</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>transformers</td>\n",
       "      <td>2023-02-25 15:37:17+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wlad777/test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wlad777</td>\n",
       "      <td>10+9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>region:us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-23 08:04:39+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VAZaytsev/ppo_LL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VAZaytsev</td>\n",
       "      <td># PPO Agent Playing LunarLander-v2\\n\\n  This i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tensorboard, LunarLander-v2, ppo, deep-reinfor...</td>\n",
       "      <td>reinforcement-learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-12 14:24:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cardiffnlp/twitter-xlm-roberta-base-sentiment-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cardiffnlp</td>\n",
       "      <td># cardiffnlp/twitter-xlm-roberta-base-sentimen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48289</td>\n",
       "      <td>28</td>\n",
       "      <td>transformers, pytorch, xlm-roberta, text-class...</td>\n",
       "      <td>text-classification</td>\n",
       "      <td>transformers</td>\n",
       "      <td>2022-12-01 00:32:11+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alireza7/ARMAN-SS-100-persian-base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alireza7</td>\n",
       "      <td>More information about models is available here.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>transformers, pytorch, pegasus, text2text-gene...</td>\n",
       "      <td>text2text-generation</td>\n",
       "      <td>transformers</td>\n",
       "      <td>2022-03-02 23:29:05+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embedding_model = \"jinaai/jina-embeddings-v3\"\n",
    "\n",
    "FIELD_WEIGHTS = {\n",
    "    'model_id': 0.05,  # Low relevance – just an identifier, rarely informative for semantic search.\n",
    "    'base_model': 0.10,  # Medium relevance – useful to understand the model's architecture or foundation.\n",
    "    'author': 0.05,  # Low relevance – rarely impacts model capabilities or domain.\n",
    "    'license': 0.05,  # Low relevance – important for legal use but not for content relevance.\n",
    "    'language': 0.10,  # Medium relevance – essential when queries specify language preferences.\n",
    "    'tags': 0.20,  # High relevance – concise, curated keywords help capture the model's purpose.\n",
    "    'pipeline_tag': 0.15,  # High relevance – explicitly defines the task (e.g., classification, QA).\n",
    "    'library_name': 0.10,  # Medium relevance – relevant when a specific framework is required.\n",
    "    'readme_file': 0.20  # Highest relevance – contains descriptive context but may be noisy or verbose.\n",
    "}"
   ],
   "id": "84dfae2a45c5edad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_weighted_embeddings(data, batch_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModel.from_pretrained(embedding_model, trust_remote_code=True).to(device)\n",
    "\n",
    "    text_columns = list(FIELD_WEIGHTS.keys())\n",
    "    data[text_columns] = data[text_columns].fillna(\"\").astype(str)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for start in tqdm(range(0, len(data), batch_size), desc=\"Processing Weighted Embeddings\"):\n",
    "        end = min(start + batch_size, len(data))\n",
    "        batch_data = data.iloc[start:end]\n",
    "\n",
    "        # Build flat list of texts to embed and keep mapping\n",
    "        texts_to_embed = []\n",
    "        index_map = []  # (row_index_in_batch, field_name)\n",
    "\n",
    "        for i, row in batch_data.iterrows():\n",
    "            for field in text_columns:\n",
    "                text = row[field].strip()\n",
    "                if text:\n",
    "                    texts_to_embed.append(text)\n",
    "                    index_map.append((i - start, field))  # i-start: relative index in batch\n",
    "\n",
    "        try:\n",
    "            # Batch encode \n",
    "            with torch.no_grad():\n",
    "                all_embeddings = model.encode(texts_to_embed, task=\"text-matching\")\n",
    "\n",
    "            # Build empty structures for rows\n",
    "            row_embs = [[] for _ in range(len(batch_data))]\n",
    "            row_weights = [0.0 for _ in range(len(batch_data))]\n",
    "\n",
    "            # Populate embeddings and weights\n",
    "            for emb, (row_idx, field) in zip(all_embeddings, index_map):\n",
    "                weight = FIELD_WEIGHTS[field]\n",
    "                row_embs[row_idx].append(np.array(emb) * weight)\n",
    "                row_weights[row_idx] += weight\n",
    "\n",
    "            # Compute final embedding per row\n",
    "            for i in range(len(batch_data)):\n",
    "                if row_embs[i]:\n",
    "                    combined_emb = np.sum(row_embs[i], axis=0) / row_weights[i]\n",
    "                else:\n",
    "                    combined_emb = len(all_embeddings[0]) if all_embeddings else 1024\n",
    "                embeddings.append(combined_emb.tolist())\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during processing the batch:{start}-{end}. Exception: {e}\")\n",
    "            print(\"Embeddings will be filled with empty lists.\")\n",
    "            embeddings.extend([[]] * (end - start))\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    data['embeddings'] = embeddings\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "df = generate_weighted_embeddings(df, batch_size=EMBEDDINGS_BATCH_SIZE)"
   ],
   "id": "804fba7bed26a315"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.head()\n",
    "df.to_csv(\"Output/datasets_hg_weighted_emb.csv\", index=False)"
   ],
   "id": "9d18eaa3be45f9b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Classic Embeddings Approach\n",
    "In this section, we will generate embeddings for the datasets using a classic approach. This involves concatenating the text from multiple fields into a single string and generating embeddings for that combined text. The embeddings will be generated using the same model as before: [jinaai/jina-embeddings-v3](https://huggingface.co/jinaai/jina-embeddings-v3)."
   ],
   "id": "88dc54b4262a181b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embedding_model = \"jinaai/jina-embeddings-v3\"\n",
    "\n",
    "\n",
    "def generate_content_embeddings(data, batch_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModel.from_pretrained(embedding_model, trust_remote_code=True).to(device)\n",
    "\n",
    "    # Assicura che tutte le colonne coinvolte siano stringhe, anche se sono liste o NaN\n",
    "    text_columns = ['model_id', 'base_model', 'author', 'license', 'language', 'tags', 'pipeline_tag', 'library_name',\n",
    "                    'readme_file']\n",
    "\n",
    "    for col in text_columns:\n",
    "        data[col] = data[col].astype(str)\n",
    "\n",
    "    data[text_columns] = data[text_columns].fillna(\"\").astype(str)\n",
    "\n",
    "    data['full_text'] = (\n",
    "            data['model_id'] + \"\\n\" +\n",
    "            data['base_model'] + \"\\n\" +\n",
    "            data['author'] + \"\\n\" +\n",
    "            data['license'] + \"\\n\" +\n",
    "            data['language'] + \"\\n\" +\n",
    "            data['tags'] + \"\\n\" +\n",
    "            data['pipeline_tag'] + \"\\n\" +\n",
    "            data['library_name'] + \"\\n\" +\n",
    "            data['readme_file']\n",
    "    )\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for start in tqdm(range(0, len(data), batch_size), desc=\"Processing Embeddings Batches\"):\n",
    "        end = min(start + batch_size, len(data))\n",
    "\n",
    "        try:\n",
    "            batch_texts = data['full_text'].iloc[start:end].tolist()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = model.encode(batch_texts, task=\"text-matching\").tolist()\n",
    "\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during processing the batch:{start}-{end}. Exception: {e}\")\n",
    "            print(\"Embeddings will be filled with empty lists.\")\n",
    "            embeddings.extend([[]] * (end - start))\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    data['embeddings'] = embeddings\n",
    "    data = data.drop(columns=['full_text'])\n",
    "    return data"
   ],
   "id": "1836ff17801b75ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = generate_content_embeddings(df, batch_size=5)\n",
    "df.head()"
   ],
   "id": "ae079e6895bd4045"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.to_csv(\"Output/models_hg_embeddings.csv\", index=False)",
   "id": "8e741317f753fe82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
