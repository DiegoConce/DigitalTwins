{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import tempfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from huggingface_hub import HfApi, list_models, model_info, hf_hub_download\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "b55518abe8917fa1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Acquisition: Building the Dataset through the Hugging Face Hub API\n",
    "\n",
    "### Overview\n",
    "The goal of this step is to construct a robust and informative dataset containing metadata for every model hosted on the Hugging Face Hub. This dataset will serve as a ground truth for a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "### The Hugging Face Hub API\n",
    "The [Hugging Face Hub API](https://huggingface.co/docs/huggingface_hub/package_reference/hf_api) provides programmatic access to the platform's extensive model repository through several endpoints, each serving different purposes and offering varying levels of detail.<br>\n",
    "The `list_models()` method allows iteration over ModelInfo objects but does not return complete metadata, even with the <b>full=True</b> parameter. To access all relevant information, we must call [`model_info()`](https://huggingface.co/docs/huggingface_hub/v0.32.3/en/package_reference/hf_api#huggingface_hub.ModelInfo) individually for each model ID. This yields full metadata, but at the cost of a large number of API requests, which can impact performance and scalability.\n",
    "\n",
    "----\n",
    "\n",
    "### Metadata Fields\n",
    "The fields that we can retrieve and those that are most useful for our purposes include:\n",
    "- **model_id**: Unique identifier for the model.\n",
    "- **base_model**: Identifier of the base model from which this model derives (e.g., for fine-tuned models).\n",
    "- **author**: The creator or organization behind the model.\n",
    "- **license**: Licensing information for the model.\n",
    "- **language**: Language of the model's training data or metadata.\n",
    "- **downloads**: Number of times the model has been downloaded.\n",
    "- **likes**: Number of likes the model has received.\n",
    "- **tags**: Tags associated with the model for easier categorization.\n",
    "- **pipeline_tag**: The pipeline tag associated with the model (e.g., text-generation, image-classification).\n",
    "- **library_name**: The library name associated with the model (e.g., transformers, diffusers).\n",
    "- **created_at**: Timestamp of when the model was created.\n",
    "- **readme_file**: The readme file of the model repository, which may contain additional context and information about the model.\n",
    "\n",
    "---\n",
    "\n",
    "### The Challenge of Context\n",
    "While the metadata fields provide valuable insights, they often lack sufficient context to fully understand the model's capabilities, limitations, and training methodology. The readme file of each model repository is a crucial resource for this additional context, but it comes with its own set of challenges:\n",
    "- **Inconsistency**: Not all models have a readme file, and those that do may vary significantly in content quality and relevance.\n",
    "- **Information Overload**: Some readme files may contain excessive or irrelevant information, making it difficult to extract useful insights.\n",
    "- **Lack of Control**: The content of readme files is user-generated, so we cannot guarantee the presence or quality of information.\n",
    "- **Performance**: Downloading readme files for a large number of models can be time-consuming and resource-intensive."
   ],
   "id": "f08d5af020b34c1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configuration\n",
    "HF_TOKEN = \"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\"\n",
    "MAX_WORKERS = 5\n",
    "BATCH_SIZE = 100\n",
    "CHECKPOINT_FILE = \"models_scraping_checkpoint.json\"\n",
    "MODEL_LIMIT = 1100\n",
    "# Cache Directory\n",
    "CHACE_DIR = \"temp_cache\"\n",
    "os.makedirs(CHACE_DIR, exist_ok=True)\n",
    "TEMP_CACHE_DIR = tempfile.mkdtemp(prefix=\"hf_temp_cache_\", dir=CHACE_DIR)\n",
    "# Generating Embeddings Batch Size\n",
    "EMBEDDINGS_BATCH_SIZE = 200"
   ],
   "id": "3b97a71aa6e3a2a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_readme_from_repository(repository_id: str):\n",
    "    try:\n",
    "        readme_content = hf_hub_download(\n",
    "            repo_id=repository_id,\n",
    "            filename=\"README.md\",\n",
    "            token=HF_TOKEN,\n",
    "            cache_dir=TEMP_CACHE_DIR\n",
    "        )\n",
    "\n",
    "        with open(readme_content, \"r\", encoding=\"utf-8\") as f:\n",
    "            readme_text = f.read()\n",
    "\n",
    "        return readme_text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download README for {repository_id}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_single_model(model_id: str) -> Optional[Dict]:\n",
    "    \"\"\"Process a single model and extract its information\"\"\"\n",
    "    try:\n",
    "        info = model_info(model_id, token=HF_TOKEN)\n",
    "        card_data = info.cardData if hasattr(info, 'cardData') and info.cardData else {}\n",
    "\n",
    "        readme = get_readme_from_repository(model_id)\n",
    "\n",
    "        return {\n",
    "            'model_id': model_id,\n",
    "            'base_model': getattr(card_data, 'base_model', None),\n",
    "            'author': getattr(info, 'author', None),\n",
    "            'readme_file': readme,\n",
    "            'license': getattr(card_data, 'license', None),\n",
    "            'language': getattr(card_data, 'language', None),\n",
    "            'downloads': getattr(info, 'downloads', 0),\n",
    "            'likes': getattr(info, 'likes', 0),\n",
    "            'tags': ', '.join(info.tags) if hasattr(info, 'tags') and info.tags else '',\n",
    "            'pipeline_tag': getattr(info, 'pipeline_tag', None),\n",
    "            'library_name': getattr(info, 'library_name', None),\n",
    "            'created_at': getattr(info, 'created_at', None),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {model_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_batch_threaded(model_ids: List[str]) -> List[Dict]:\n",
    "    \"\"\"Process a batch of models using ThreadPoolExecutor\"\"\"\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_model = {\n",
    "            executor.submit(process_single_model, model_id): model_id\n",
    "            for model_id in model_ids\n",
    "        }\n",
    "\n",
    "        # Collect results with progress bar\n",
    "        for future in tqdm(as_completed(future_to_model),\n",
    "                          total=len(model_ids),\n",
    "                          desc=f\"Processing batch\"):\n",
    "            try:\n",
    "                result = future.result(timeout=60)  # 60 second timeout\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                model_id = future_to_model[future]\n",
    "                logger.error(f\"Timeout/Error for {model_id}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_checkpoint(data):\n",
    "    # Save checkpoint\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'w') as f:\n",
    "            json.dump(data, f, indent=2, default=str)\n",
    "        logging.info(f\"Checkpoint saved: {len(data)} datasets processed\")\n",
    "        print(f\"Progress: {len(data)}/{len(datasets)} datasets completed\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save checkpoint: {e}\")\n",
    "\n",
    "\n",
    "def cleanup_temp_cache():\n",
    "    try:\n",
    "        if os.path.exists(TEMP_CACHE_DIR):\n",
    "            shutil.rmtree(TEMP_CACHE_DIR)\n",
    "            logging.info(f\"Current temporary cache {TEMP_CACHE_DIR} deleted.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not delete current temp cache dir {TEMP_CACHE_DIR}: {e}\")\n",
    "\n",
    "\n",
    "def clean_all_cache_folders():\n",
    "    try:\n",
    "        if os.path.exists(CHACE_DIR):\n",
    "            for item in os.listdir(CHACE_DIR):\n",
    "                item_path = os.path.join(CHACE_DIR, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    shutil.rmtree(item_path)\n",
    "                    logging.info(f\"Deleted cache folder: {item_path}\")\n",
    "                else:\n",
    "                    os.remove(item_path)\n",
    "                    logging.info(f\"Deleted cache file: {item_path}\")\n",
    "\n",
    "            logging.info(f\"All cache folders in {CHACE_DIR} cleaned up.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not delete cache folders in {CHACE_DIR}: {e}\")"
   ],
   "id": "7daa8949e8e5fdd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load Checkpoint (if exists)\n",
    "checkpoint_data = []\n",
    "start_index = 0\n",
    "\n",
    "if Path(CHECKPOINT_FILE).exists():\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        start_index = len(checkpoint_data)\n",
    "        print(f\"Loaded checkpoint: {start_index} models already processed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load checkpoint file: {e}\")\n",
    "else:\n",
    "    print(\"No checkpoint file found. Starting from scratch.\")"
   ],
   "id": "fb070de8af96d7a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Fetching models from Hugging Face Hub...\")\n",
    "models = list(list_models(limit=MODEL_LIMIT))\n",
    "print(f\"Fetched {len(models)} models\")\n",
    "\n",
    "# Prepare models to process\n",
    "models_to_process = models[start_index:]\n",
    "all_data = checkpoint_data.copy()\n",
    "\n",
    "print(f\"Models remaining to process: {len(models_to_process)}\")"
   ],
   "id": "c2ff60bfbc58d3cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Process Models in Batches\n",
    "if models_to_process:\n",
    "    total_batches = (len(models_to_process) - 1) // BATCH_SIZE + 1\n",
    "\n",
    "    for i in range(0, len(models_to_process), BATCH_SIZE):\n",
    "        batch_models = models_to_process[i:i + BATCH_SIZE]\n",
    "        batch_ids = [m.modelId for m in batch_models]\n",
    "\n",
    "        current_batch = i // BATCH_SIZE + 1\n",
    "        print(f\"\\nProcessing batch {current_batch}/{total_batches}\")\n",
    "        print(f\"Batch size: {len(batch_ids)} models\")\n",
    "\n",
    "        # Process batch\n",
    "        batch_results = process_batch_threaded(batch_ids)\n",
    "        all_data.extend(batch_results)\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(all_data)\n",
    "\n",
    "        # Clean up temporary temp_cache directory\n",
    "        cleanup_temp_cache()\n",
    "\n",
    "        # Rate limiting\n",
    "        time.sleep(0.5)"
   ],
   "id": "62b30884a9d51fd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"\\nScraping completed!\")\n",
    "print(f\"Total models processed: {len(df)}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ],
   "id": "cfff885db55dba8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cleaning Markdown Readme Files\n",
    "This step involves cleaning the readme files extracted from the Hugging Face models to ensure that they contain only relevant textual content, while preserving titles and important information. The cleaning process will remove unnecessary formatting, images, links, and other non-essential elements."
   ],
   "id": "2329f6cca3134533"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_markdown(text):\n",
    "    \"\"\"\n",
    "    Clean markdown text and extract only textual content while preserving titles.\n",
    "\n",
    "    Args:\n",
    "        markdown_text (str): Raw markdown text\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text with titles preserved\n",
    "        :param text: the markdown text to clean\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove YAML front matter (--- ... ---)\n",
    "    text = re.sub(r'^---\\s*\\n.*?\\n---\\s*\\n', '', text, flags=re.DOTALL | re.MULTILINE)\n",
    "\n",
    "    # Remove HTML tags but keep the content\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove images ![alt text](url) or ![alt text][ref]\n",
    "    text = re.sub(r'!\\[.*?\\]\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r'!\\[.*?\\]\\[[^\\]]*\\]', '', text)\n",
    "\n",
    "    # Remove standalone image references [image]: url\n",
    "    text = re.sub(r'^\\s*\\[.*?\\]:\\s*https?://.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Convert headers to plain text (preserve titles)\n",
    "    # Handle # ## ### #### ##### ###### headers\n",
    "    text = re.sub(r'^#{1,6}\\s+(.+)$', r'\\1', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove links but keep the link text [text](url) -> text\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]\\([^)]*\\)', r'\\1', text)\n",
    "\n",
    "    # Remove reference-style links [text][ref] -> text\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]\\[[^\\]]*\\]', r'\\1', text)\n",
    "\n",
    "    # Remove link references [ref]: url\n",
    "    text = re.sub(r'^\\s*\\[.*?\\]:\\s*.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove code blocks (triple backticks)\n",
    "    text = re.sub(r'```[\\s\\S]*?```', '', text)\n",
    "\n",
    "    # Remove inline code `code` -> code\n",
    "    text = re.sub(r'`([^`]*)`', r'\\1', text)\n",
    "\n",
    "    # Remove bold and italic formatting\n",
    "    text = re.sub(r'\\*\\*\\*(.*?)\\*\\*\\*', r'\\1', text)  # Bold italic\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)  # Bold\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)  # Italic\n",
    "    text = re.sub(r'___(.*?)___', r'\\1', text)  # Bold italic\n",
    "    text = re.sub(r'__(.*?)__', r'\\1', text)  # Bold\n",
    "    text = re.sub(r'_(.*?)_', r'\\1', text)  # Italic\n",
    "\n",
    "    # Remove strikethrough ~~text~~ -> text\n",
    "    text = re.sub(r'~~(.*?)~~', r'\\1', text)\n",
    "\n",
    "    # Remove blockquotes > text -> text\n",
    "    text = re.sub(r'^>\\s*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove horizontal rules (--- or ***)\n",
    "    text = re.sub(r'^[-*]{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove list markers (-, *, +, numbers)\n",
    "    text = re.sub(r'^[\\s]*[-*+]\\s+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^[\\s]*\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove tables (simple approach - remove lines with | characters)\n",
    "    text = re.sub(r'^.*\\|.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Unescape HTML entities (much more comprehensive than manual replacement)\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Clean up extra whitespace\n",
    "    # Remove empty lines with only whitespace\n",
    "    text = re.sub(r'^\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove multiple consecutive newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "    # Remove leading/trailing whitespace from each line\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "\n",
    "    # Remove empty lines but preserve paragraph structure\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if line:  # Only keep non-empty lines\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    # Join lines back together with single newlines\n",
    "    result = '\\n'.join(cleaned_lines)\n",
    "\n",
    "    # Remove any remaining multiple newlines\n",
    "    result = re.sub(r'\\n{2,}', '\\n', result)\n",
    "\n",
    "    # Final cleanup\n",
    "    result = result.strip()\n",
    "\n",
    "    return result"
   ],
   "id": "4f7507a9b6fcef4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Readme empty string count:\", (df['readme_file'] == '').sum())\n",
    "print(\"Removing models with empty readme...\")\n",
    "df = df[df['readme_file'] != '']\n",
    "print(\"DataFrame shape after removing empty readmes:\", df.shape)\n",
    "print(\"Applying markdown cleaning to readme files\")\n",
    "df['readme_file'] = df['readme_file'].apply(clean_markdown)\n",
    "print(\"Final shape of DataFrame:\", df.shape)\n",
    "df.head()"
   ],
   "id": "adcbb2302c5ee22c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generating Content Embeddings\n",
    "This is a preprocessing step in order to generate embeddings for the content of the models. The embeddings will be used to compare and rank models based on their metadata and readme content, enabling efficient retrieval in a RAG system. The model used is an open source model: [jinaai/jina-embeddings-v3](https://huggingface.co/jinaai/jina-embeddings-v3)\n",
    "\n",
    "A few important considerations:\n",
    "- The embedding model is not specifically trained on structured metadata fields (e.g. license, tags), so it may not fully capture their semantic weight or relevance.\n",
    "- README files often contain noisy, inconsistent, or sparse information. This can affect the quality of the resulting embeddings.\n",
    "- Some fields such as `license`,`language` or `tags` might be missing or incomplete.\n",
    "\n",
    "At this stage, we apply a simple approach: we concatenate all available metadata fields along with the README content into a single string, which is then embedded. <br>\n",
    "Maybe a more valuable approach could be to use a weighted sum of embeddings, where each field has a different weight based on its importance or relevance to the model's capabilities."
   ],
   "id": "ae644642af27146d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "embedding_model = \"jinaai/jina-embeddings-v3\"\n",
    "\n",
    "def generate_content_embeddings(data, batch_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModel.from_pretrained(embedding_model, trust_remote_code=True).to(device)\n",
    "\n",
    "    # Assicura che tutte le colonne coinvolte siano stringhe, anche se sono liste o NaN\n",
    "    text_columns = ['model_id', 'base_model', 'author', 'license', 'language', 'tags', 'pipeline_tag', 'library_name', 'readme_file']\n",
    "\n",
    "    for col in text_columns:\n",
    "        data[col] = data[col].astype(str)\n",
    "\n",
    "    data[text_columns] = data[text_columns].fillna(\"\").astype(str)\n",
    "\n",
    "\n",
    "    data['full_text'] = (\n",
    "        data['model_id'] + \"\\n\" +\n",
    "        data['base_model'] + \"\\n\" +\n",
    "        data['author'] + \"\\n\" +\n",
    "        data['license'] + \"\\n\" +\n",
    "        data['language'] + \"\\n\" +\n",
    "        data['tags'] + \"\\n\" +\n",
    "        data['pipeline_tag'] + \"\\n\" +\n",
    "        data['library_name'] + \"\\n\" +\n",
    "        data['readme_file']\n",
    "    )\n",
    "\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for start in tqdm(range(0, len(data), batch_size), desc=\"Processing Embeddings Batches\"):\n",
    "        end = min(start + batch_size, len(data))\n",
    "\n",
    "        try:\n",
    "            batch_texts = data['full_text'].iloc[start:end].tolist()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = model.encode(batch_texts, task=\"text-matching\").tolist()\n",
    "\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during processing the batch:{start}-{end}. Exception: {e}\")\n",
    "            print(\"Embeddings will be filled with empty lists.\")\n",
    "            embeddings.extend([[]] * (end - start))\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    data['embeddings'] = embeddings\n",
    "    data = data.drop(columns=['full_text'])\n",
    "    return data"
   ],
   "id": "1836ff17801b75ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = generate_content_embeddings(df, batch_size=5)\n",
    "df.head()"
   ],
   "id": "ae079e6895bd4045"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.to_csv(\"huggingface_models_embeddings.csv\", index=False)",
   "id": "8e741317f753fe82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
