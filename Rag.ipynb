{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import ast\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ],
   "id": "3c9f4164116e0590"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RAG: Query based retrival using embeddings\n",
    "### Overview\n",
    "The goal of this step is to filter and rank articles based on their embedding similarity scores to a user-provided prompt. This is done using a pre-trained model to convert the prompt into embeddings, and then comparing these embeddings with the embeddings of articles stored in a DataFrame. <br>\n",
    "\n",
    "The open source model is the same used before in the preprocessing step, which is [`jinaai/jina-embeddings-v3`](https://huggingface.co/jinaai/jina-embeddings-v3).\n",
    "\n",
    "#### Possibile future improvements\n",
    "The current filtering approach based on user queries can be expanded by incorporating additional criteria such as number of downloads, likes, or publication date. This a simple implementation which does not require any computational resources.<br>\n",
    "Moreover, the retrieval performance can be improved by experimenting with alternative similarity metrics such as cosine similarity or Euclidean distance or trying with another model.\n"
   ],
   "id": "1876f56ec763ad81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "system_content_prompt = \"\"\"\n",
    "As an intelligent language model, your role is to accurately determine whether the provided data is relevant to the user's query.\n",
    "Answer ONLY with 'Yes' or 'No'\n",
    "\"\"\"\n",
    "language_model = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "embedding_model = \"jinaai/jina-embeddings-v3\"\n",
    "\n",
    "\n",
    "def convert_prompt_to_embedding(prompt):\n",
    "    \"\"\"\n",
    "    Converts text prompt to embeddings using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input text to convert to embeddings\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Embedding vector representation of the input text\n",
    "    \"\"\"\n",
    "    model = AutoModel.from_pretrained(embedding_model, trust_remote_code=True).to(\"cpu\")  # for now cpu\n",
    "    embedding = model.encode(prompt, task=\"text-matching\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def compute_score(embeddings, prompt):\n",
    "    models_embedding = np.array(embeddings)\n",
    "    return np.dot(models_embedding, prompt)\n",
    "\n",
    "\n",
    "def filter_by_score(data, prompt, range=10):\n",
    "    \"\"\"\n",
    "    Filters and ranks models based on embedding similarity scores.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing 'embeddings' column\n",
    "        prompt (numpy.ndarray): Prompt embedding to compare against\n",
    "        range (int, optional): Number of top models to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Top N articles sorted by similarity score\n",
    "    \"\"\"\n",
    "\n",
    "    data['embeddings'] = data['embeddings'].apply(\n",
    "        lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else x)  #da sistemare\n",
    "\n",
    "    data['score'] = data['embeddings'].progress_apply(lambda x: compute_score(x, prompt))\n",
    "    data.sort_values(by='score', ascending=False, inplace=True)\n",
    "    return data.head(range)\n",
    "\n",
    "\n",
    "def build_user_content(data, mode=\"model\"):\n",
    "    content = \"\"\n",
    "\n",
    "    if mode == \"model\":\n",
    "        content = data['model_id'] + \"\\n\" + data['base_model'] + \"\\n\" + data['author'] + \"\\n\" + data[\n",
    "            'readme_file'] + \"\\n\" + data['license'] + \"\\n\" + data['language'] + \"\\n\" + data['tags'] + \"\\n\" + data[\n",
    "                      'pipeline_tag'] + \"\\n\" + data['library_name']\n",
    "\n",
    "    elif mode == \"dataset\":\n",
    "        content = data['dataset_id'] + \"\\n\" + data['author'] + \"\\n\" + data['readme_file'] + \"\\n\" + data[\n",
    "            'tags'] + \"\\n\" + data['language'] + \"\\n\" + data['license'] + \"\\n\" + data['multilinguality'] + \"\\n\" + data[\n",
    "                      'size_categories'] + \"\\n\" + data['task-categories']\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def filter_by_user_prompt(data, user_prompt, mode=\"model\"):\n",
    "    prompt_embedding = convert_prompt_to_embedding(user_prompt)\n",
    "    data = filter_by_score(data, prompt_embedding)\n",
    "\n",
    "    indices_to_remove = []\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(language_model, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        language_model,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto'\n",
    "    )\n",
    "\n",
    "    for i, item in tqdm(data.iterrows(), total=len(data), desc=\"Evaluating data relevance\"):\n",
    "        user_content = build_user_content(data, mode)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_content_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ]\n",
    "\n",
    "        tokenized = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        tokenized = tokenizer(tokenized, return_tensors='pt').to('cuda')\n",
    "        generated_ids = model.generate(**tokenized, max_new_tokens=3000)\n",
    "        output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        result = output[-2:]\n",
    "        print(result)\n",
    "\n",
    "        if \"no\" in result.lower():\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "        if indices_to_remove:\n",
    "            data = data.drop(indices_to_remove)\n",
    "            data = data.reset_index(drop=True)\n",
    "\n",
    "    print(f\"Shortlisted {len(data)} relevant items\")\n",
    "\n",
    "    return data"
   ],
   "id": "27027777e4ab3a50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"huggingface_models_embeddings.csv\")\n",
    "df.head()"
   ],
   "id": "58d01174e4dbc2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example with models from Hugging Face",
   "id": "529bd4e785684a05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model prompts\n",
    "user_prompts = [\n",
    "    \"What is the best model for text generation?\",\n",
    "    \"Which model should I use for sentiment analysis?\",\n",
    "    \"Find top models for image classification.\",\n",
    "    \"Best models for summarization tasks?\",\n",
    "    \"What are the newest models for code generation?\",\n",
    "    \"Top-performing models for question answering?\",\n",
    "    \"Which models support Italian language?\",\n",
    "    \"Best lightweight models for mobile deployment.\",\n",
    "    \"Which models are most popular on Hugging Face?\",\n",
    "    \"Find models optimized for speed and low latency.\"\n",
    "]\n",
    "\n",
    "# Log results\n",
    "with open(\"models_results_log.txt\", \"w\", encoding=\"utf-8\") as log_file:\n",
    "    for i, prompt in enumerate(user_prompts, 1):\n",
    "        log_file.write(f\"\\n--- Query {i}: {prompt} ---\\n\")\n",
    "        filtered_df = filter_by_user_prompt(df, prompt, mode=\"model\")\n",
    "        for _, row in filtered_df.iterrows():\n",
    "            log_file.write(f\"\\n{row['model_id']} | score: {row['score']:.4f}\\n\")\n",
    "            log_file.write(f\"Author: {row['author']}\\n\")\n",
    "            log_file.write(f\"Pipeline Tag: {row['pipeline_tag']}\\n\")\n",
    "            log_file.write(f\"ReadmeFile, first 100 characters: {row['readme_file'][:100]}\\n\")\n",
    "            log_file.write(\"-\" * 50 + \"\\n\")\n",
    "        log_file.write(\"\\n\")"
   ],
   "id": "51e4786f9ee4e946"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example with datasets from Hugging Face",
   "id": "8fea03b106c256b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"Output/datasets_hg_embeddings.csv\")\n",
    "print(df.shape)"
   ],
   "id": "ecd77864b3416b9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head()",
   "id": "180fa3423ac57585"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dataset prompts\n",
    "user_prompts = [\n",
    "    \"Which datasets are best for sentiment analysis in Italian?\",\n",
    "    \"Find large-scale datasets with multilingual support and open licenses.\",\n",
    "    \"What are the top trending datasets for text summarization?\",\n",
    "    \"Datasets with detailed README files and active contributors.\",\n",
    "    \"Show datasets suitable for low-resource language modeling.\",\n",
    "    \"Which datasets support image classification tasks and are under 1GB?\",\n",
    "    \"List recently created datasets for question answering in biomedical domain.\",\n",
    "    \"Find datasets curated for cross-lingual classification tasks with labeled examples and language identifiers.\",\n",
    "    \"Find high-quality datasets for code generation with permissive licenses.\",\n",
    "    \"List benchmark NLP datasets commonly used in academic research and model evaluation.\"\n",
    "]\n",
    "\n",
    "# Log results\n",
    "with open(\"datasets_results_log.txt\", \"w\", encoding=\"utf-8\") as log_file:\n",
    "    for i, prompt in enumerate(user_prompts, 1):\n",
    "        log_file.write(f\"\\n--- Query {i}: {prompt} ---\\n\")\n",
    "        filtered_df = filter_by_user_prompt(df, prompt, mode=\"dataset\")\n",
    "        for _, row in filtered_df.iterrows():\n",
    "            log_file.write(f\"\\n{row['dataset_id']} | score: {row['score']:.4f}\\n\")\n",
    "            log_file.write(f\"Author: {row['author']}\\n\")\n",
    "            log_file.write(f\"Task Categories: {row['task-categories']}\\n\")\n",
    "            log_file.write(f\"ReadmeFile, first 100 characters: {row['readme_file'][:100]}\\n\")\n",
    "            # divider\n",
    "            log_file.write(\"-\" * 50 + \"\\n\")\n",
    "        log_file.write(\"\\n\")"
   ],
   "id": "514b988347b15488"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
