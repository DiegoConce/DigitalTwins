{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-10T20:50:05.035396Z",
     "start_time": "2025-06-10T20:50:03.129878Z"
    }
   },
   "source": [
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import html\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download, list_datasets\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\miniconda3\\envs\\DigitalTwins\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Acquisition: Building a Dataset from Hugging Face Hub API\n",
    "\n",
    "### Overview\n",
    "The goal of this step is to construct a robust and informative dataset containing metadata for every dataset hosted on the Hugging Face Hub. This dataset will serve as a ground truth for a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "### The Hugging Face Hub API\n",
    "The [Hugging Face Hub API](https://huggingface.co/docs/huggingface_hub/package_reference/hf_api) provides programmatic access to the platform's extensive model repository through several endpoints, each serving different purposes and offering varying levels of detail.<br>\n",
    "The [`list_datasets()`](https://huggingface.co/docs/huggingface_hub/v0.32.3/en/package_reference/hf_api#huggingface_hub.HfApi.list_datasets) method allows iteration over DatasetInfo objects, with the use of the `full=True` parameter we can retrive all the metadata associated with each dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Metadata Fields\n",
    "The fields that we can retrieve and those that are most useful for our purposes include:\n",
    "- **dataset_id**: Unique identifier for the dataset.\n",
    "- **author**: The creator or maintainer of the dataset.\n",
    "- **created_at**: Timestamp indicating when the dataset was created.\n",
    "- **readme_file**: The readme file of the model repository, which may contain additional context and information about the model.\n",
    "- **downloads**: Number of times the dataset has been downloaded.\n",
    "- **likes**: Number of likes the dataset has received.\n",
    "- **tags**: Tags associated with the dataset, useful for categorization.\n",
    "- **language**: Language of dataset’s data or metadata.\n",
    "- **license**: License under which the dataset is released.\n",
    "- **multilinguality**: Whether the dataset is multilingual. Options are: ‘monolingual’, ‘multilingual’, ‘translation’, ‘other’.\n",
    "- **size_categories**: The number of examples in the dataset. Options are: ‘n<1K’, ‘1K1T’, and ‘other’.\n",
    "- **task-categories**: The tasks the dataset is intended for, such as ‘text-classification’, ‘text-generation’, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### The Challenge of Context\n",
    "While the metadata fields provide valuable insights, they often lack the necessary context to fully understand the dataset's purpose and content. The readme file is a crucial component that can fill this gap, offering detailed explanations, usage examples, and additional information that is not captured in the metadata alone, but it comes with its own set of challenges that we've seen before:\n",
    "- **Inconsistency**: Not all models have a readme file, and those that do may vary significantly in content quality and relevance.\n",
    "- **Information Overload**: Some readme files may contain excessive or irrelevant information, making it difficult to extract useful insights.\n",
    "- **Lack of Control**: The content of readme files is user-generated, so we cannot guarantee the presence or quality of information.\n",
    "- **Performance**: Downloading readme files for a large number of models can be time-consuming and resource-intensive."
   ],
   "id": "2fe953f5ccaae70a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T20:50:05.051550Z",
     "start_time": "2025-06-10T20:50:05.047366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "HF_TOKEN = \"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\"\n",
    "MAX_WORKERS = 5\n",
    "BATCH_SIZE = 10\n",
    "CHECKPOINT_FILE = \"dataset_scraping_checkpoint.json\"\n",
    "MODEL_LIMIT = 100\n",
    "# Cache Directory\n",
    "CHACE_DIR = \"temp_cache\"\n",
    "os.makedirs(CHACE_DIR, exist_ok=True)\n",
    "TEMP_CACHE_DIR = tempfile.mkdtemp(prefix=\"hf_temp_cache_\", dir=CHACE_DIR)\n",
    "# Generating Embeddings Batch Size\n",
    "EMBEDDINGS_BATCH_SIZE = 200"
   ],
   "id": "bc60e813480af66a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T20:50:05.529826Z",
     "start_time": "2025-06-10T20:50:05.518489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_readme_from_repository(repository_id: str):\n",
    "    try:\n",
    "        readme_content = hf_hub_download(\n",
    "            repo_id=repository_id,\n",
    "            filename=\"README.md\",\n",
    "            token=HF_TOKEN,\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=TEMP_CACHE_DIR\n",
    "        )\n",
    "\n",
    "        with open(readme_content, \"r\", encoding=\"utf-8\") as f:\n",
    "            readme_text = f.read()\n",
    "\n",
    "        return readme_text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download README for {repository_id}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def process_single_dataset(dataset_info):\n",
    "    try:\n",
    "        dataset_id = dataset_info.id\n",
    "        card_data = dataset_info.cardData if hasattr(dataset_info, 'cardData') else {}\n",
    "        readme_text = get_readme_from_repository(dataset_id)\n",
    "\n",
    "        return {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"author\": getattr(dataset_info, 'author', None),\n",
    "            \"created_at\": getattr(dataset_info, 'created_at', None),\n",
    "            \"readme_file\": readme_text,\n",
    "            \"downloads\": getattr(dataset_info, 'downloads', 0),\n",
    "            \"likes\": getattr(dataset_info, 'likes', 0),\n",
    "            \"tags\": getattr(dataset_info, 'tags', None),\n",
    "            \"language\": getattr(card_data, 'language', None),\n",
    "            \"license\": getattr(card_data, 'license', None),\n",
    "            \"multilinguality\": getattr(card_data, 'multilinguality', None),\n",
    "            \"size_categories\": getattr(card_data, 'size_categories', None),\n",
    "            \"task-categories\": getattr(card_data, 'task_categories', None),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing dataset {dataset_info.id}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_batch_threaded(dataset_list):\n",
    "    \"\"\"Process a batch of dataset info objects using ThreadPoolExecutor\"\"\"\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_dataset = {\n",
    "            executor.submit(process_single_dataset, dataset_info): dataset_info.id\n",
    "            for dataset_info in dataset_list\n",
    "        }\n",
    "\n",
    "        # Collect results with progress bar\n",
    "        for future in tqdm(as_completed(future_to_dataset),\n",
    "                           total=len(dataset_list),\n",
    "                           desc=f\"Processing batch\"):\n",
    "            try:\n",
    "                result = future.result(timeout=60)  # 60 second timeout\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                dataset_id = future_to_dataset[future]\n",
    "                logger.error(f\"Timeout/Error for {dataset_id}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_checkpoint(data):\n",
    "    # Save checkpoint\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'w') as f:\n",
    "            json.dump(data, f, indent=2, default=str)\n",
    "        logging.info(f\"Checkpoint saved: {len(data)} datasets processed\")\n",
    "        print(f\"Progress: {len(data)}/{len(datasets)} datasets completed\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save checkpoint: {e}\")\n",
    "\n",
    "\n",
    "def cleanup_temp_cache():\n",
    "    try:\n",
    "        if os.path.exists(TEMP_CACHE_DIR):\n",
    "            shutil.rmtree(TEMP_CACHE_DIR)\n",
    "            logging.info(f\"Current temporary cache {TEMP_CACHE_DIR} deleted.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not delete current temp cache dir {TEMP_CACHE_DIR}: {e}\")\n",
    "\n",
    "\n",
    "def clean_all_cache_folders():\n",
    "    try:\n",
    "        if os.path.exists(CHACE_DIR):\n",
    "            for item in os.listdir(CHACE_DIR):\n",
    "                item_path = os.path.join(CHACE_DIR, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    shutil.rmtree(item_path)\n",
    "                    logging.info(f\"Deleted cache folder: {item_path}\")\n",
    "                else:\n",
    "                    os.remove(item_path)\n",
    "                    logging.info(f\"Deleted cache file: {item_path}\")\n",
    "\n",
    "            logging.info(f\"All cache folders in {CHACE_DIR} cleaned up.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not delete cache folders in {CHACE_DIR}: {e}\")\n"
   ],
   "id": "31260b238da24ad4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T20:52:37.827263Z",
     "start_time": "2025-06-10T20:52:34.174639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Checkpoint (if exists)\n",
    "checkpoint_data = []\n",
    "start_index = 0\n",
    "\n",
    "if Path(CHECKPOINT_FILE).exists():\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        start_index = len(checkpoint_data)\n",
    "        print(f\"Loaded checkpoint: {start_index} datasets already processed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load checkpoint file: {e}\")\n",
    "else:\n",
    "    print(\"No checkpoint file found. Starting from scratch.\")"
   ],
   "id": "b90b58da3112d595",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: 162400 datasets already processed\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:44:25.258157Z",
     "start_time": "2025-06-10T12:44:24.881867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Fetching dataset info from Hugging Face Hub...\")\n",
    "datasets = list(list_datasets(limit=MODEL_LIMIT, full=True))\n",
    "print(f\"Total datasets found: {len(datasets)}\")\n",
    "\n",
    "datasets_to_process = datasets[start_index:]\n",
    "all_data = checkpoint_data.copy()\n",
    "\n",
    "print(f\"Datasets remaining to process: {len(datasets_to_process)}\")"
   ],
   "id": "72c5348effe03ae8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching dataset info from Hugging Face Hub...\n",
      "Total datasets found: 100\n",
      "Datasets remaining to process: 100\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:44:43.363403Z",
     "start_time": "2025-06-10T12:44:25.267807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if datasets_to_process:\n",
    "    total_batches = (len(datasets_to_process) - 1) // BATCH_SIZE + 1\n",
    "\n",
    "    for i in range(0, len(datasets_to_process), BATCH_SIZE):\n",
    "        batch_datasets = datasets_to_process[i:i + BATCH_SIZE]\n",
    "\n",
    "        current_batch = i // BATCH_SIZE + 1\n",
    "        print(f\"\\nProcessing batch {current_batch}/{total_batches}\")\n",
    "        print(f\"Batch size: {len(batch_datasets)} datasets\")\n",
    "\n",
    "        # Process batch\n",
    "        batch_results = process_batch_threaded(batch_datasets)\n",
    "        all_data.extend(batch_results)\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(all_data)\n",
    "\n",
    "        # Clean up temporary temp_cache directory\n",
    "        cleanup_temp_cache()\n",
    "\n",
    "        # Rate limiting\n",
    "        time.sleep(0.5)"
   ],
   "id": "cb46ace0069719db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [00:01<00:00,  8.44it/s]\n",
      "INFO:root:Checkpoint saved: 10 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10/100 datasets completed\n",
      "\n",
      "Processing batch 2/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [00:01<00:00,  6.92it/s]\n",
      "INFO:root:Checkpoint saved: 20 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 20/100 datasets completed\n",
      "\n",
      "Processing batch 3/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [00:01<00:00,  7.13it/s]\n",
      "INFO:root:Checkpoint saved: 30 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 30/100 datasets completed\n",
      "\n",
      "Processing batch 4/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [00:01<00:00,  5.00it/s]\n",
      "INFO:root:Checkpoint saved: 40 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 40/100 datasets completed\n",
      "\n",
      "Processing batch 5/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [00:01<00:00,  7.46it/s]\n",
      "INFO:root:Checkpoint saved: 50 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50/100 datasets completed\n",
      "\n",
      "Processing batch 6/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [00:01<00:00,  7.24it/s]\n",
      "INFO:root:Checkpoint saved: 60 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 60/100 datasets completed\n",
      "\n",
      "Processing batch 7/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [00:01<00:00,  9.90it/s]\n",
      "INFO:root:Checkpoint saved: 70 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 70/100 datasets completed\n",
      "\n",
      "Processing batch 8/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [00:00<00:00, 12.98it/s]\n",
      "INFO:root:Checkpoint saved: 80 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 80/100 datasets completed\n",
      "\n",
      "Processing batch 9/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/10 [00:00<?, ?it/s]ERROR:__main__:Failed to download README for mamachang/medical-reasoning: 404 Client Error. (Request ID: Root=1-684828b7-7bb400e765d1d4bf58d3535d;edffb06a-ffcb-43d1-be5c-187ccf2df6b1)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/datasets/mamachang/medical-reasoning/resolve/main/README.md.\n",
      "Processing batch: 100%|██████████| 10/10 [00:01<00:00,  8.18it/s]\n",
      "INFO:root:Checkpoint saved: 90 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 90/100 datasets completed\n",
      "\n",
      "Processing batch 10/10\n",
      "Batch size: 10 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 10/10 [00:00<00:00, 10.58it/s]\n",
      "INFO:root:Checkpoint saved: 100 datasets processed\n",
      "INFO:root:Current temporary cache C:\\Workspace\\DigitalTwins\\temp_cache\\hf_temp_cache_x1qys5ba deleted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100/100 datasets completed\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:44:43.383450Z",
     "start_time": "2025-06-10T12:44:43.374230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"\\nScraping completed!\")\n",
    "print(f\"Total datasets processed: {len(df)}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Cleaning up all cache folders...\")\n",
    "clean_all_cache_folders()"
   ],
   "id": "22d1f596b73754e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:All cache folders in temp_cache cleaned up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completed!\n",
      "Total datasets processed: 100\n",
      "Dataset shape: (100, 12)\n",
      "Cleaning up all cache folders...\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cleaning Markdown Readme Files\n",
    "This step involves cleaning the readme files extracted from the Hugging Face models to ensure that they contain only relevant textual content, while preserving titles and important information. The cleaning process will remove unnecessary formatting, images, links, and other non-essential elements."
   ],
   "id": "9fab43a749604974"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:44:57.129375Z",
     "start_time": "2025-06-10T12:44:57.070483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_markdown(text):\n",
    "    # Remove YAML front matter\n",
    "    text = re.sub(r'^---.*?---\\s*', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove images (![alt](url))\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Remove markdown links but keep the visible text: [text](url) → text\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "\n",
    "    # Remove tables (lines containing |, excluding bullet points)\n",
    "    text = re.sub(r'^\\s*\\|.*\\|.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove code blocks (``` ... ```)\n",
    "    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove inline code (`code`)\n",
    "    text = re.sub(r'`[^`]+`', '', text)\n",
    "\n",
    "    # Remove blockquotes (> ...)\n",
    "    text = re.sub(r'^\\s*>.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove citation block (```)\n",
    "    text = re.sub(r'^@misc.*?```', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove extra newlines\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "\n",
    "    # Trim whitespace\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Clean the readme text in the DataFrame\n",
    "df['readme_file'] = df['readme_file'].apply(clean_markdown)"
   ],
   "id": "c725fd8f38cff749",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:44:59.026595Z",
     "start_time": "2025-06-10T12:44:58.995485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Readme empty string count:\", (df['readme_file'] == '').sum())\n",
    "print(\"Removing models with empty readme...\")\n",
    "df = df[df['readme_file'] != '']\n",
    "print(\"DataFrame shape after removing empty readmes:\", df.shape)\n",
    "print(\"Applying markdown cleaning to readme files\")\n",
    "df['readme_file'] = df['readme_file'].apply(clean_markdown)\n",
    "print(\"Final shape of DataFrame:\", df.shape)\n",
    "df.head()"
   ],
   "id": "7e36dd90e6e14ef3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readme empty string count: 8\n",
      "Removing models with empty readme...\n",
      "DataFrame shape after removing empty readmes: (92, 12)\n",
      "Applying markdown cleaning to readme files\n",
      "Final shape of DataFrame: (92, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                               dataset_id         author  \\\n",
       "0        open-thoughts/OpenThoughts3-1.2M  open-thoughts   \n",
       "1  a-m-team/AM-DeepSeek-R1-0528-Distilled       a-m-team   \n",
       "2             fka/awesome-chatgpt-prompts            fka   \n",
       "3                           yandex/yambda         yandex   \n",
       "4                       Hcompany/WebClick       Hcompany   \n",
       "\n",
       "                 created_at  \\\n",
       "0 2025-05-28 21:51:11+00:00   \n",
       "1 2025-06-04 01:50:01+00:00   \n",
       "2 2022-12-13 23:47:45+00:00   \n",
       "3 2025-05-27 10:41:39+00:00   \n",
       "4 2025-04-30 19:44:42+00:00   \n",
       "\n",
       "                                         readme_file  downloads  likes  \\\n",
       "0  paper |\\ndataset |\\nmodel\\n\\n \\n\\n# OpenThough...       3695     75   \n",
       "1  ## 📘 Dataset Summary\\n\\nThis dataset is a high...       2501     46   \n",
       "2  🧠 Awesome ChatGPT Prompts [CSV dataset]\\n\\nThi...      20949   7893   \n",
       "3  # Yambda-5B — A Large-Scale Multi-modal Datase...      39566    154   \n",
       "4  # WebClick: A Multimodal Localization Benchmar...       3557     45   \n",
       "\n",
       "                                                tags  language     license  \\\n",
       "0  [task_categories:text-generation, license:apac...      None  apache-2.0   \n",
       "1  [task_categories:text-generation, language:en,...  [en, zh]        None   \n",
       "2  [task_categories:question-answering, license:c...      None     cc0-1.0   \n",
       "3  [license:apache-2.0, size_categories:1B<n<10B,...      None  apache-2.0   \n",
       "4  [task_categories:visual-document-retrieval, la...      [en]  apache-2.0   \n",
       "\n",
       "  multilinguality size_categories              task-categories  \n",
       "0            None            None            [text-generation]  \n",
       "1            None      [1M<n<10M]            [text-generation]  \n",
       "2            None     [100K<n<1M]         [question-answering]  \n",
       "3            None      [1B<n<10B]                         None  \n",
       "4            None            None  [visual-document-retrieval]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_at</th>\n",
       "      <th>readme_file</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>tags</th>\n",
       "      <th>language</th>\n",
       "      <th>license</th>\n",
       "      <th>multilinguality</th>\n",
       "      <th>size_categories</th>\n",
       "      <th>task-categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>open-thoughts/OpenThoughts3-1.2M</td>\n",
       "      <td>open-thoughts</td>\n",
       "      <td>2025-05-28 21:51:11+00:00</td>\n",
       "      <td>paper |\\ndataset |\\nmodel\\n\\n \\n\\n# OpenThough...</td>\n",
       "      <td>3695</td>\n",
       "      <td>75</td>\n",
       "      <td>[task_categories:text-generation, license:apac...</td>\n",
       "      <td>None</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[text-generation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a-m-team/AM-DeepSeek-R1-0528-Distilled</td>\n",
       "      <td>a-m-team</td>\n",
       "      <td>2025-06-04 01:50:01+00:00</td>\n",
       "      <td>## 📘 Dataset Summary\\n\\nThis dataset is a high...</td>\n",
       "      <td>2501</td>\n",
       "      <td>46</td>\n",
       "      <td>[task_categories:text-generation, language:en,...</td>\n",
       "      <td>[en, zh]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[1M&lt;n&lt;10M]</td>\n",
       "      <td>[text-generation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fka/awesome-chatgpt-prompts</td>\n",
       "      <td>fka</td>\n",
       "      <td>2022-12-13 23:47:45+00:00</td>\n",
       "      <td>🧠 Awesome ChatGPT Prompts [CSV dataset]\\n\\nThi...</td>\n",
       "      <td>20949</td>\n",
       "      <td>7893</td>\n",
       "      <td>[task_categories:question-answering, license:c...</td>\n",
       "      <td>None</td>\n",
       "      <td>cc0-1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>[100K&lt;n&lt;1M]</td>\n",
       "      <td>[question-answering]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yandex/yambda</td>\n",
       "      <td>yandex</td>\n",
       "      <td>2025-05-27 10:41:39+00:00</td>\n",
       "      <td># Yambda-5B — A Large-Scale Multi-modal Datase...</td>\n",
       "      <td>39566</td>\n",
       "      <td>154</td>\n",
       "      <td>[license:apache-2.0, size_categories:1B&lt;n&lt;10B,...</td>\n",
       "      <td>None</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>[1B&lt;n&lt;10B]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hcompany/WebClick</td>\n",
       "      <td>Hcompany</td>\n",
       "      <td>2025-04-30 19:44:42+00:00</td>\n",
       "      <td># WebClick: A Multimodal Localization Benchmar...</td>\n",
       "      <td>3557</td>\n",
       "      <td>45</td>\n",
       "      <td>[task_categories:visual-document-retrieval, la...</td>\n",
       "      <td>[en]</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[visual-document-retrieval]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generating Content Embeddings\n",
    "This is a preprocessing step in order to generate embeddings for the content of the models. The embeddings will be used to compare and rank models based on their metadata and readme content, enabling efficient retrieval in a RAG system. The model used is an open source model: [jinaai/jina-embeddings-v3](https://huggingface.co/jinaai/jina-embeddings-v3)"
   ],
   "id": "4507c20562435cc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T12:44:44.984378500Z",
     "start_time": "2025-06-10T12:34:51.807393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "embedding_model = \"jinaai/jina-embeddings-v3\"\n",
    "\n",
    "\n",
    "def generate_content_embeddings(data, batch_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModel.from_pretrained(embedding_model, trust_remote_code=True).to(device)\n",
    "\n",
    "    text_columns = ['dataset_id', 'author', 'created_at', 'readme',\n",
    "                    'tags', 'language', 'license', 'multilinguality', 'size_categories',\n",
    "                    'task-categories']\n",
    "\n",
    "    for col in text_columns:\n",
    "        data[col] = data[col].astype(str)\n",
    "\n",
    "    data[text_columns] = data[text_columns].fillna(\"\").astype(str)\n",
    "\n",
    "    data['full_text'] = (\n",
    "            data['dataset_id'] + \"\\n\" +\n",
    "            data['author'] + \"\\n\" +\n",
    "            data['created_at'] + \"\\n\" +\n",
    "            data['readme'] + \"\\n\" +\n",
    "            data['tags'] + \"\\n\" +\n",
    "            data['language'] + \"\\n\" +\n",
    "            data['license'] + \"\\n\" +\n",
    "            data['multilinguality'] + \"\\n\" +\n",
    "            data['size_categories'] + \"\\n\" +\n",
    "            data['task-categories']\n",
    "    )\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for start in tqdm(range(0, len(data), batch_size), desc=\"Processing Embeddings Batches\"):\n",
    "        end = min(start + batch_size, len(data))\n",
    "\n",
    "    try:\n",
    "        batch_texts = data['full_text'].iloc[start:end].tolist()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode(batch_texts, task=\"text-matching\").tolist()\n",
    "\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing the batch:{start}-{end}. Exception: {e}\")\n",
    "        print(\"Embeddings will be filled with empty lists.\")\n",
    "        embeddings.extend([[]] * (end - start))\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    data['embeddings'] = embeddings\n",
    "    data = data.drop(columns=['full_text'])\n",
    "    return data"
   ],
   "id": "e818965e82635212",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Automodel' from 'transformers' (C:\\Users\\Diego\\miniconda3\\envs\\DigitalTwins\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[140]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Automodel\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgc\u001B[39;00m\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'Automodel' from 'transformers' (C:\\Users\\Diego\\miniconda3\\envs\\DigitalTwins\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = generate_content_embeddings(df, batch_size=EMBEDDINGS_BATCH_SIZE)\n",
    "df.head()"
   ],
   "id": "1dab161df1ec6ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T08:10:57.051210Z",
     "start_time": "2025-06-11T08:10:56.599245Z"
    }
   },
   "cell_type": "code",
   "source": "df.to_csv(\"Output/datasets_hg_embeddings.csv\", index=False)",
   "id": "de2ac647753c043e",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df.to_csv(\u001B[33m\"\u001B[39m\u001B[33mOutput/datasets_hg_embeddings.csv\u001B[39m\u001B[33m\"\u001B[39m, index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[31mNameError\u001B[39m: name 'df' is not defined"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
